{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "#-----------------------------------\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "#-----------------------------------\n",
    "import numpy as np\n",
    "#-----------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.constraints import max_norm \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "#-------------------------------------\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7 \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sonar_df1 = pd.read_csv(\"sonar.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   R  \n",
       "1    0.0052  0.0044   R  \n",
       "2    0.0095  0.0078   R  \n",
       "3    0.0040  0.0117   R  \n",
       "4    0.0107  0.0094   R  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   M  \n",
       "204  0.0062  0.0067   M  \n",
       "205  0.0077  0.0031   M  \n",
       "206  0.0036  0.0048   M  \n",
       "207  0.0061  0.0115   M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sonar_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   1  \n",
       "1    0.0052  0.0044   1  \n",
       "2    0.0095  0.0078   1  \n",
       "3    0.0040  0.0117   1  \n",
       "4    0.0107  0.0094   1  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   0  \n",
       "204  0.0062  0.0067   0  \n",
       "205  0.0077  0.0031   0  \n",
       "206  0.0036  0.0048   0  \n",
       "207  0.0061  0.0115   0  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = pp.LabelEncoder()\n",
    "\n",
    "Sonar_df1.iloc[ : , 60] = enc.fit_transform(Sonar_df1.iloc[ : , 60])\n",
    "Sonar_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_df = shuffle(Sonar_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0628</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.2734</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.2231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.1163</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2417</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>0.5378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.2365</td>\n",
       "      <td>0.2461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.2984</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
       "128  0.0374  0.0586  0.0628  0.0534  0.0255  0.1422  0.2072  0.2734  0.3070   \n",
       "158  0.0107  0.0453  0.0289  0.0713  0.1075  0.1019  0.1606  0.2119  0.3061   \n",
       "98   0.1313  0.2339  0.3059  0.4264  0.4010  0.1791  0.1853  0.0055  0.1929   \n",
       "131  0.1150  0.1163  0.0866  0.0358  0.0232  0.1267  0.2417  0.2661  0.4346   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "151  0.0231  0.0315  0.0170  0.0226  0.0410  0.0116  0.0223  0.0805  0.2365   \n",
       "67   0.0368  0.0403  0.0317  0.0293  0.0820  0.1342  0.1161  0.0663  0.0155   \n",
       "25   0.0201  0.0026  0.0138  0.0062  0.0133  0.0151  0.0541  0.0210  0.0505   \n",
       "196  0.0050  0.0017  0.0270  0.0450  0.0958  0.0830  0.0879  0.1220  0.1977   \n",
       "175  0.0294  0.0123  0.0117  0.0113  0.0497  0.0998  0.1326  0.1117  0.2984   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "11   0.0835  ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
       "128  0.2597  ...  0.0118  0.0063  0.0237  0.0032  0.0087  0.0124  0.0113   \n",
       "158  0.2936  ...  0.0164  0.0120  0.0113  0.0021  0.0097  0.0072  0.0060   \n",
       "98   0.2231  ...  0.0362  0.0210  0.0154  0.0180  0.0013  0.0106  0.0127   \n",
       "131  0.5378  ...  0.0099  0.0065  0.0085  0.0166  0.0110  0.0190  0.0141   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "151  0.2461  ...  0.0125  0.0036  0.0123  0.0043  0.0114  0.0052  0.0091   \n",
       "67   0.0506  ...  0.0091  0.0160  0.0160  0.0081  0.0070  0.0135  0.0067   \n",
       "25   0.1097  ...  0.0108  0.0070  0.0063  0.0030  0.0011  0.0007  0.0024   \n",
       "196  0.2282  ...  0.0165  0.0056  0.0010  0.0027  0.0062  0.0024  0.0063   \n",
       "175  0.3473  ...  0.0056  0.0104  0.0079  0.0014  0.0054  0.0015  0.0006   \n",
       "\n",
       "         58      59  60  \n",
       "11   0.0009  0.0044   1  \n",
       "128  0.0098  0.0126   0  \n",
       "158  0.0017  0.0036   0  \n",
       "98   0.0178  0.0231   0  \n",
       "131  0.0068  0.0086   0  \n",
       "..      ...     ...  ..  \n",
       "151  0.0008  0.0092   0  \n",
       "67   0.0078  0.0068   1  \n",
       "25   0.0057  0.0044   1  \n",
       "196  0.0017  0.0028   0  \n",
       "175  0.0081  0.0043   0  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sonar_df.iloc[ : 144, :60]\n",
    "df_train_labels = sonar_df.iloc[ :144, 60]\n",
    "\n",
    "df_validation =  sonar_df.iloc[144:165 , :60]\n",
    "df_validation_labels = sonar_df.iloc[144:165, 60]\n",
    "\n",
    "df_test = sonar_df.iloc[165: , :60]\n",
    "df_test_labels = sonar_df.iloc[165: , 60]\n",
    "\n",
    "train_data = df_train.to_numpy()\n",
    "train_labels = df_train_labels.to_numpy()\n",
    "\n",
    "Validation_data = df_validation.to_numpy()\n",
    "Validation_labels = df_validation_labels.to_numpy()\n",
    "\n",
    "test_data = df_test.to_numpy()\n",
    "test_labels = df_test_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_validation_labels = to_categorical(Validation_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dropout(0.4, input_shape=(60,)),\n",
    "                                    tf.keras.layers.Dense(64, activation='relu', kernel_constraint=max_norm(3)),\n",
    "                                    tf.keras.layers.Dense(32, activation='relu', kernel_constraint=max_norm(3)),\n",
    "                                    tf.keras.layers.Dense(16, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(1, activation='sigmoid')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "sgd = SGD(lr = 0.1, momentum = 0.8, decay = 0.002, nesterov = False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144 samples, validate on 21 samples\n",
      "Epoch 1/500\n",
      "144/144 [==============================] - 6s 39ms/sample - loss: 0.7033 - accuracy: 0.4653 - val_loss: 0.6737 - val_accuracy: 0.6190\n",
      "Epoch 2/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.6991 - accuracy: 0.5417 - val_loss: 0.6706 - val_accuracy: 0.6190\n",
      "Epoch 3/500\n",
      "144/144 [==============================] - 0s 965us/sample - loss: 0.6919 - accuracy: 0.5347 - val_loss: 0.6771 - val_accuracy: 0.6190\n",
      "Epoch 4/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.6967 - accuracy: 0.4444 - val_loss: 0.6900 - val_accuracy: 0.4286\n",
      "Epoch 5/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.6751 - accuracy: 0.5972 - val_loss: 0.6495 - val_accuracy: 0.6190\n",
      "Epoch 6/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.6785 - accuracy: 0.5278 - val_loss: 0.6392 - val_accuracy: 0.6190\n",
      "Epoch 7/500\n",
      "144/144 [==============================] - 0s 785us/sample - loss: 0.6749 - accuracy: 0.5694 - val_loss: 0.6420 - val_accuracy: 0.9524\n",
      "Epoch 8/500\n",
      "144/144 [==============================] - 0s 931us/sample - loss: 0.6690 - accuracy: 0.5764 - val_loss: 0.6094 - val_accuracy: 0.6667\n",
      "Epoch 9/500\n",
      "144/144 [==============================] - 0s 924us/sample - loss: 0.6413 - accuracy: 0.6667 - val_loss: 0.5806 - val_accuracy: 0.7143\n",
      "Epoch 10/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.6496 - accuracy: 0.6181 - val_loss: 0.5700 - val_accuracy: 0.6667\n",
      "Epoch 11/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.7031 - accuracy: 0.6181 - val_loss: 0.5927 - val_accuracy: 0.8095\n",
      "Epoch 12/500\n",
      "144/144 [==============================] - 0s 931us/sample - loss: 0.6299 - accuracy: 0.6736 - val_loss: 0.6036 - val_accuracy: 0.7143\n",
      "Epoch 13/500\n",
      "144/144 [==============================] - 0s 910us/sample - loss: 0.6231 - accuracy: 0.6736 - val_loss: 0.5686 - val_accuracy: 0.7143\n",
      "Epoch 14/500\n",
      "144/144 [==============================] - 0s 958us/sample - loss: 0.6123 - accuracy: 0.6250 - val_loss: 0.5334 - val_accuracy: 0.8571\n",
      "Epoch 15/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5931 - accuracy: 0.6944 - val_loss: 0.5474 - val_accuracy: 0.6667\n",
      "Epoch 16/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.5969 - accuracy: 0.6458 - val_loss: 0.5250 - val_accuracy: 0.7143\n",
      "Epoch 17/500\n",
      "144/144 [==============================] - 0s 951us/sample - loss: 0.5903 - accuracy: 0.6806 - val_loss: 0.5668 - val_accuracy: 0.7619\n",
      "Epoch 18/500\n",
      "144/144 [==============================] - 0s 896us/sample - loss: 0.6174 - accuracy: 0.6389 - val_loss: 0.5283 - val_accuracy: 0.6667\n",
      "Epoch 19/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.6307 - accuracy: 0.6181 - val_loss: 0.5475 - val_accuracy: 0.7143\n",
      "Epoch 20/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.6282 - accuracy: 0.6528 - val_loss: 0.5753 - val_accuracy: 0.8571\n",
      "Epoch 21/500\n",
      "144/144 [==============================] - 0s 958us/sample - loss: 0.6270 - accuracy: 0.6944 - val_loss: 0.6229 - val_accuracy: 0.6667\n",
      "Epoch 22/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.6163 - accuracy: 0.6667 - val_loss: 0.5456 - val_accuracy: 0.7143\n",
      "Epoch 23/500\n",
      "144/144 [==============================] - 0s 806us/sample - loss: 0.6247 - accuracy: 0.6181 - val_loss: 0.5200 - val_accuracy: 0.7619\n",
      "Epoch 24/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.6018 - accuracy: 0.6944 - val_loss: 0.5456 - val_accuracy: 0.8571\n",
      "Epoch 25/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.6024 - accuracy: 0.6667 - val_loss: 0.5133 - val_accuracy: 0.8095\n",
      "Epoch 26/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.5681 - accuracy: 0.7222 - val_loss: 0.5010 - val_accuracy: 0.8095\n",
      "Epoch 27/500\n",
      "144/144 [==============================] - 0s 799us/sample - loss: 0.5758 - accuracy: 0.6806 - val_loss: 0.4939 - val_accuracy: 0.7619\n",
      "Epoch 28/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5687 - accuracy: 0.6736 - val_loss: 0.4836 - val_accuracy: 0.8095\n",
      "Epoch 29/500\n",
      "144/144 [==============================] - 0s 840us/sample - loss: 0.5757 - accuracy: 0.7014 - val_loss: 0.5002 - val_accuracy: 0.7143\n",
      "Epoch 30/500\n",
      "144/144 [==============================] - 0s 806us/sample - loss: 0.5091 - accuracy: 0.7639 - val_loss: 0.4613 - val_accuracy: 0.8571\n",
      "Epoch 31/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.6103 - accuracy: 0.6597 - val_loss: 0.4639 - val_accuracy: 0.7619\n",
      "Epoch 32/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.5494 - accuracy: 0.7292 - val_loss: 0.4334 - val_accuracy: 0.8095\n",
      "Epoch 33/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5519 - accuracy: 0.7292 - val_loss: 0.4537 - val_accuracy: 0.6667\n",
      "Epoch 34/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5877 - accuracy: 0.6736 - val_loss: 0.4603 - val_accuracy: 0.9048\n",
      "Epoch 35/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.5321 - accuracy: 0.7153 - val_loss: 0.4263 - val_accuracy: 0.8571\n",
      "Epoch 36/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4983 - accuracy: 0.7847 - val_loss: 0.4130 - val_accuracy: 0.9048\n",
      "Epoch 37/500\n",
      "144/144 [==============================] - 0s 924us/sample - loss: 0.4868 - accuracy: 0.7500 - val_loss: 0.4099 - val_accuracy: 0.9048\n",
      "Epoch 38/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.6274 - accuracy: 0.6597 - val_loss: 0.4497 - val_accuracy: 0.9048\n",
      "Epoch 39/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.5279 - accuracy: 0.7639 - val_loss: 0.4703 - val_accuracy: 0.8571\n",
      "Epoch 40/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.5785 - accuracy: 0.7292 - val_loss: 0.4610 - val_accuracy: 0.9048\n",
      "Epoch 41/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.5385 - accuracy: 0.7431 - val_loss: 0.4389 - val_accuracy: 0.7619\n",
      "Epoch 42/500\n",
      "144/144 [==============================] - 0s 889us/sample - loss: 0.5264 - accuracy: 0.6944 - val_loss: 0.4181 - val_accuracy: 0.9048\n",
      "Epoch 43/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.4994 - accuracy: 0.7708 - val_loss: 0.4023 - val_accuracy: 0.8571\n",
      "Epoch 44/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.5431 - accuracy: 0.7292 - val_loss: 0.4310 - val_accuracy: 0.7143\n",
      "Epoch 45/500\n",
      "144/144 [==============================] - 0s 833us/sample - loss: 0.5041 - accuracy: 0.7569 - val_loss: 0.4280 - val_accuracy: 0.8095\n",
      "Epoch 46/500\n",
      "144/144 [==============================] - 0s 806us/sample - loss: 0.5080 - accuracy: 0.7431 - val_loss: 0.4471 - val_accuracy: 0.7619\n",
      "Epoch 47/500\n",
      "144/144 [==============================] - 0s 729us/sample - loss: 0.5167 - accuracy: 0.7361 - val_loss: 0.4477 - val_accuracy: 0.8571\n",
      "Epoch 48/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.5436 - accuracy: 0.6944 - val_loss: 0.4485 - val_accuracy: 0.7143\n",
      "Epoch 49/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5319 - accuracy: 0.7083 - val_loss: 0.4582 - val_accuracy: 0.8095\n",
      "Epoch 50/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5446 - accuracy: 0.7153 - val_loss: 0.4428 - val_accuracy: 0.8095\n",
      "Epoch 51/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5555 - accuracy: 0.7014 - val_loss: 0.4372 - val_accuracy: 0.7619\n",
      "Epoch 52/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.5685 - accuracy: 0.6875 - val_loss: 0.4556 - val_accuracy: 0.9524\n",
      "Epoch 53/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5293 - accuracy: 0.7778 - val_loss: 0.4358 - val_accuracy: 0.9048\n",
      "Epoch 54/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5130 - accuracy: 0.7083 - val_loss: 0.4065 - val_accuracy: 0.9048\n",
      "Epoch 55/500\n",
      "144/144 [==============================] - 0s 819us/sample - loss: 0.5471 - accuracy: 0.7153 - val_loss: 0.3691 - val_accuracy: 0.9524\n",
      "Epoch 56/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5530 - accuracy: 0.7361 - val_loss: 0.3762 - val_accuracy: 0.9524\n",
      "Epoch 57/500\n",
      "144/144 [==============================] - 0s 576us/sample - loss: 0.5369 - accuracy: 0.7153 - val_loss: 0.4165 - val_accuracy: 0.9524\n",
      "Epoch 58/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.5425 - accuracy: 0.7500 - val_loss: 0.4286 - val_accuracy: 0.8571\n",
      "Epoch 59/500\n",
      "144/144 [==============================] - 0s 597us/sample - loss: 0.5005 - accuracy: 0.7639 - val_loss: 0.3971 - val_accuracy: 0.8095\n",
      "Epoch 60/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.5122 - accuracy: 0.7292 - val_loss: 0.3841 - val_accuracy: 0.8571\n",
      "Epoch 61/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.5919 - accuracy: 0.6389 - val_loss: 0.4062 - val_accuracy: 0.9048\n",
      "Epoch 62/500\n",
      "144/144 [==============================] - 0s 583us/sample - loss: 0.5237 - accuracy: 0.7292 - val_loss: 0.4126 - val_accuracy: 0.9048\n",
      "Epoch 63/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4923 - accuracy: 0.7500 - val_loss: 0.3970 - val_accuracy: 0.9524\n",
      "Epoch 64/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.5367 - accuracy: 0.7153 - val_loss: 0.3994 - val_accuracy: 0.9048\n",
      "Epoch 65/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4608 - accuracy: 0.7639 - val_loss: 0.4299 - val_accuracy: 0.7143\n",
      "Epoch 66/500\n",
      "144/144 [==============================] - 0s 868us/sample - loss: 0.4650 - accuracy: 0.7708 - val_loss: 0.4335 - val_accuracy: 0.8095\n",
      "Epoch 67/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5496 - accuracy: 0.7083 - val_loss: 0.4021 - val_accuracy: 0.7619\n",
      "Epoch 68/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5704 - accuracy: 0.6736 - val_loss: 0.4129 - val_accuracy: 0.8571\n",
      "Epoch 69/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5017 - accuracy: 0.7431 - val_loss: 0.4000 - val_accuracy: 0.9048\n",
      "Epoch 70/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5327 - accuracy: 0.7222 - val_loss: 0.3947 - val_accuracy: 0.9048\n",
      "Epoch 71/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.5400 - accuracy: 0.6736 - val_loss: 0.4498 - val_accuracy: 0.8571\n",
      "Epoch 72/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.5364 - accuracy: 0.7083 - val_loss: 0.4053 - val_accuracy: 0.9048\n",
      "Epoch 73/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4702 - accuracy: 0.7847 - val_loss: 0.3796 - val_accuracy: 0.9048\n",
      "Epoch 74/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.5468 - accuracy: 0.7153 - val_loss: 0.3821 - val_accuracy: 0.9048\n",
      "Epoch 75/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.5578 - accuracy: 0.7014 - val_loss: 0.3928 - val_accuracy: 0.9524\n",
      "Epoch 76/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.5105 - accuracy: 0.7569 - val_loss: 0.3885 - val_accuracy: 0.9048\n",
      "Epoch 77/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.5751 - accuracy: 0.7153 - val_loss: 0.4095 - val_accuracy: 0.8095\n",
      "Epoch 78/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4767 - accuracy: 0.7431 - val_loss: 0.4030 - val_accuracy: 0.9048\n",
      "Epoch 79/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.5151 - accuracy: 0.7014 - val_loss: 0.3740 - val_accuracy: 0.8571\n",
      "Epoch 80/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.5390 - accuracy: 0.7222 - val_loss: 0.3877 - val_accuracy: 0.8571\n",
      "Epoch 81/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4919 - accuracy: 0.7222 - val_loss: 0.3811 - val_accuracy: 0.9048\n",
      "Epoch 82/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4851 - accuracy: 0.8056 - val_loss: 0.3817 - val_accuracy: 0.9048\n",
      "Epoch 83/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4775 - accuracy: 0.7569 - val_loss: 0.3723 - val_accuracy: 0.9048\n",
      "Epoch 84/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4876 - accuracy: 0.7153 - val_loss: 0.3681 - val_accuracy: 0.8571\n",
      "Epoch 85/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.5469 - accuracy: 0.7153 - val_loss: 0.3602 - val_accuracy: 0.9048\n",
      "Epoch 86/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.5157 - accuracy: 0.7361 - val_loss: 0.3696 - val_accuracy: 0.9048\n",
      "Epoch 87/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.5611 - accuracy: 0.7083 - val_loss: 0.3832 - val_accuracy: 0.9524\n",
      "Epoch 88/500\n",
      "144/144 [==============================] - 0s 785us/sample - loss: 0.4873 - accuracy: 0.7361 - val_loss: 0.4025 - val_accuracy: 0.9048\n",
      "Epoch 89/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.5054 - accuracy: 0.7500 - val_loss: 0.3836 - val_accuracy: 0.9048\n",
      "Epoch 90/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.5927 - accuracy: 0.6667 - val_loss: 0.4119 - val_accuracy: 0.9048\n",
      "Epoch 91/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.5119 - accuracy: 0.7917 - val_loss: 0.4163 - val_accuracy: 0.8571\n",
      "Epoch 92/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4405 - accuracy: 0.7847 - val_loss: 0.3672 - val_accuracy: 0.9048\n",
      "Epoch 93/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.5202 - accuracy: 0.7569 - val_loss: 0.3437 - val_accuracy: 0.9048\n",
      "Epoch 94/500\n",
      "144/144 [==============================] - 0s 757us/sample - loss: 0.5469 - accuracy: 0.7292 - val_loss: 0.3854 - val_accuracy: 0.8095\n",
      "Epoch 95/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4585 - accuracy: 0.7778 - val_loss: 0.3876 - val_accuracy: 0.8571\n",
      "Epoch 96/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.5661 - accuracy: 0.6597 - val_loss: 0.4055 - val_accuracy: 0.9048\n",
      "Epoch 97/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.5144 - accuracy: 0.7569 - val_loss: 0.4305 - val_accuracy: 0.8095\n",
      "Epoch 98/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.5469 - accuracy: 0.7361 - val_loss: 0.4340 - val_accuracy: 0.8095\n",
      "Epoch 99/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.5350 - accuracy: 0.7153 - val_loss: 0.3974 - val_accuracy: 0.9048\n",
      "Epoch 100/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.5133 - accuracy: 0.7292 - val_loss: 0.3868 - val_accuracy: 0.8571\n",
      "Epoch 101/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4966 - accuracy: 0.7708 - val_loss: 0.3808 - val_accuracy: 0.8571\n",
      "Epoch 102/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4686 - accuracy: 0.7778 - val_loss: 0.3663 - val_accuracy: 0.8095\n",
      "Epoch 103/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.5236 - accuracy: 0.6875 - val_loss: 0.3648 - val_accuracy: 0.9048\n",
      "Epoch 104/500\n",
      "144/144 [==============================] - 0s 882us/sample - loss: 0.4873 - accuracy: 0.7708 - val_loss: 0.3959 - val_accuracy: 0.8571\n",
      "Epoch 105/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4658 - accuracy: 0.7847 - val_loss: 0.3562 - val_accuracy: 0.9048\n",
      "Epoch 106/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.5233 - accuracy: 0.7153 - val_loss: 0.3451 - val_accuracy: 0.8571\n",
      "Epoch 107/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4760 - accuracy: 0.7222 - val_loss: 0.3540 - val_accuracy: 0.9048\n",
      "Epoch 108/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4895 - accuracy: 0.7222 - val_loss: 0.3557 - val_accuracy: 0.8095\n",
      "Epoch 109/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.5398 - accuracy: 0.7222 - val_loss: 0.3655 - val_accuracy: 0.8095\n",
      "Epoch 110/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.5168 - accuracy: 0.7292 - val_loss: 0.3810 - val_accuracy: 0.8571\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 0s 688us/sample - loss: 0.5123 - accuracy: 0.7500 - val_loss: 0.3980 - val_accuracy: 0.8571\n",
      "Epoch 112/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4910 - accuracy: 0.7222 - val_loss: 0.3389 - val_accuracy: 0.9048\n",
      "Epoch 113/500\n",
      "144/144 [==============================] - 0s 569us/sample - loss: 0.4591 - accuracy: 0.8125 - val_loss: 0.3267 - val_accuracy: 0.9048\n",
      "Epoch 114/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4777 - accuracy: 0.7500 - val_loss: 0.3013 - val_accuracy: 0.9048\n",
      "Epoch 115/500\n",
      "144/144 [==============================] - 0s 576us/sample - loss: 0.5028 - accuracy: 0.7083 - val_loss: 0.3042 - val_accuracy: 0.9048\n",
      "Epoch 116/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4673 - accuracy: 0.7639 - val_loss: 0.3407 - val_accuracy: 0.8571\n",
      "Epoch 117/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4673 - accuracy: 0.7708 - val_loss: 0.3646 - val_accuracy: 0.8095\n",
      "Epoch 118/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.4860 - accuracy: 0.7639 - val_loss: 0.3600 - val_accuracy: 0.8095\n",
      "Epoch 119/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4636 - accuracy: 0.7847 - val_loss: 0.3525 - val_accuracy: 0.8095\n",
      "Epoch 120/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.4860 - accuracy: 0.7708 - val_loss: 0.3515 - val_accuracy: 0.8095\n",
      "Epoch 121/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4692 - accuracy: 0.7639 - val_loss: 0.3620 - val_accuracy: 0.8571\n",
      "Epoch 122/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4977 - accuracy: 0.7708 - val_loss: 0.4047 - val_accuracy: 0.8095\n",
      "Epoch 123/500\n",
      "144/144 [==============================] - 0s 597us/sample - loss: 0.5010 - accuracy: 0.7708 - val_loss: 0.3961 - val_accuracy: 0.7619\n",
      "Epoch 124/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4430 - accuracy: 0.7847 - val_loss: 0.3603 - val_accuracy: 0.8571\n",
      "Epoch 125/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4878 - accuracy: 0.7361 - val_loss: 0.3527 - val_accuracy: 0.8095\n",
      "Epoch 126/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4630 - accuracy: 0.7778 - val_loss: 0.3620 - val_accuracy: 0.8095\n",
      "Epoch 127/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.5501 - accuracy: 0.7083 - val_loss: 0.3488 - val_accuracy: 0.9048\n",
      "Epoch 128/500\n",
      "144/144 [==============================] - 0s 597us/sample - loss: 0.5012 - accuracy: 0.7569 - val_loss: 0.3731 - val_accuracy: 0.8095\n",
      "Epoch 129/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4921 - accuracy: 0.7639 - val_loss: 0.3985 - val_accuracy: 0.8095\n",
      "Epoch 130/500\n",
      "144/144 [==============================] - 0s 799us/sample - loss: 0.4367 - accuracy: 0.8125 - val_loss: 0.3627 - val_accuracy: 0.8571\n",
      "Epoch 131/500\n",
      "144/144 [==============================] - 0s 875us/sample - loss: 0.4703 - accuracy: 0.7917 - val_loss: 0.3354 - val_accuracy: 0.8571\n",
      "Epoch 132/500\n",
      "144/144 [==============================] - 0s 847us/sample - loss: 0.5001 - accuracy: 0.7569 - val_loss: 0.3280 - val_accuracy: 0.9048\n",
      "Epoch 133/500\n",
      "144/144 [==============================] - 0s 840us/sample - loss: 0.5589 - accuracy: 0.7569 - val_loss: 0.3613 - val_accuracy: 0.9048\n",
      "Epoch 134/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.5024 - accuracy: 0.7431 - val_loss: 0.4126 - val_accuracy: 0.8095\n",
      "Epoch 135/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4946 - accuracy: 0.7431 - val_loss: 0.3883 - val_accuracy: 0.8095\n",
      "Epoch 136/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.5097 - accuracy: 0.7014 - val_loss: 0.3833 - val_accuracy: 0.8095\n",
      "Epoch 137/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.5206 - accuracy: 0.7431 - val_loss: 0.3732 - val_accuracy: 0.8571\n",
      "Epoch 138/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4480 - accuracy: 0.8125 - val_loss: 0.3480 - val_accuracy: 0.9048\n",
      "Epoch 139/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4880 - accuracy: 0.7153 - val_loss: 0.3359 - val_accuracy: 0.9048\n",
      "Epoch 140/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4492 - accuracy: 0.8056 - val_loss: 0.3337 - val_accuracy: 0.9048\n",
      "Epoch 141/500\n",
      "144/144 [==============================] - 0s 861us/sample - loss: 0.4416 - accuracy: 0.7500 - val_loss: 0.3419 - val_accuracy: 0.8571\n",
      "Epoch 142/500\n",
      "144/144 [==============================] - 0s 771us/sample - loss: 0.4309 - accuracy: 0.7639 - val_loss: 0.3211 - val_accuracy: 0.9524\n",
      "Epoch 143/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.6197 - accuracy: 0.6736 - val_loss: 0.3869 - val_accuracy: 0.8571\n",
      "Epoch 144/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.5085 - accuracy: 0.7014 - val_loss: 0.4231 - val_accuracy: 0.7619\n",
      "Epoch 145/500\n",
      "144/144 [==============================] - 0s 896us/sample - loss: 0.5237 - accuracy: 0.7361 - val_loss: 0.4685 - val_accuracy: 0.7619\n",
      "Epoch 146/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4714 - accuracy: 0.7500 - val_loss: 0.3760 - val_accuracy: 0.9048\n",
      "Epoch 147/500\n",
      "144/144 [==============================] - 0s 882us/sample - loss: 0.4619 - accuracy: 0.7778 - val_loss: 0.3660 - val_accuracy: 0.7619\n",
      "Epoch 148/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.4737 - accuracy: 0.7847 - val_loss: 0.3622 - val_accuracy: 0.9048\n",
      "Epoch 149/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5031 - accuracy: 0.7639 - val_loss: 0.3794 - val_accuracy: 0.8095\n",
      "Epoch 150/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4571 - accuracy: 0.7569 - val_loss: 0.3673 - val_accuracy: 0.8571\n",
      "Epoch 151/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4781 - accuracy: 0.7361 - val_loss: 0.3650 - val_accuracy: 0.8095\n",
      "Epoch 152/500\n",
      "144/144 [==============================] - 0s 806us/sample - loss: 0.4515 - accuracy: 0.7847 - val_loss: 0.3592 - val_accuracy: 0.8095\n",
      "Epoch 153/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.5396 - accuracy: 0.7431 - val_loss: 0.3982 - val_accuracy: 0.8095\n",
      "Epoch 154/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4623 - accuracy: 0.7917 - val_loss: 0.4053 - val_accuracy: 0.8095\n",
      "Epoch 155/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4369 - accuracy: 0.7431 - val_loss: 0.3780 - val_accuracy: 0.7619\n",
      "Epoch 156/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5125 - accuracy: 0.7639 - val_loss: 0.3664 - val_accuracy: 0.8095\n",
      "Epoch 157/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4399 - accuracy: 0.7917 - val_loss: 0.3580 - val_accuracy: 0.8095\n",
      "Epoch 158/500\n",
      "144/144 [==============================] - 0s 903us/sample - loss: 0.5001 - accuracy: 0.7153 - val_loss: 0.3978 - val_accuracy: 0.7619\n",
      "Epoch 159/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.5426 - accuracy: 0.7222 - val_loss: 0.3568 - val_accuracy: 0.8571\n",
      "Epoch 160/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5002 - accuracy: 0.7222 - val_loss: 0.4155 - val_accuracy: 0.8095\n",
      "Epoch 161/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.5040 - accuracy: 0.7014 - val_loss: 0.4063 - val_accuracy: 0.7619\n",
      "Epoch 162/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4544 - accuracy: 0.7569 - val_loss: 0.4176 - val_accuracy: 0.8095\n",
      "Epoch 163/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4899 - accuracy: 0.7222 - val_loss: 0.3984 - val_accuracy: 0.8571\n",
      "Epoch 164/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4065 - accuracy: 0.8125 - val_loss: 0.3804 - val_accuracy: 0.8095\n",
      "Epoch 165/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.4869 - accuracy: 0.7431 - val_loss: 0.3704 - val_accuracy: 0.8095\n",
      "Epoch 166/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4975 - accuracy: 0.7778 - val_loss: 0.3492 - val_accuracy: 0.9048\n",
      "Epoch 167/500\n",
      "144/144 [==============================] - 0s 917us/sample - loss: 0.4750 - accuracy: 0.7500 - val_loss: 0.3571 - val_accuracy: 0.8571\n",
      "Epoch 168/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4671 - accuracy: 0.7778 - val_loss: 0.3974 - val_accuracy: 0.8571\n",
      "Epoch 169/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4002 - accuracy: 0.7986 - val_loss: 0.3457 - val_accuracy: 0.8571\n",
      "Epoch 170/500\n",
      "144/144 [==============================] - 0s 896us/sample - loss: 0.4819 - accuracy: 0.7639 - val_loss: 0.3669 - val_accuracy: 0.8571\n",
      "Epoch 171/500\n",
      "144/144 [==============================] - 0s 861us/sample - loss: 0.4746 - accuracy: 0.7708 - val_loss: 0.3607 - val_accuracy: 0.8571\n",
      "Epoch 172/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4567 - accuracy: 0.7917 - val_loss: 0.3738 - val_accuracy: 0.8095\n",
      "Epoch 173/500\n",
      "144/144 [==============================] - 0s 993us/sample - loss: 0.4508 - accuracy: 0.7847 - val_loss: 0.3557 - val_accuracy: 0.9048\n",
      "Epoch 174/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.4757 - accuracy: 0.7708 - val_loss: 0.3768 - val_accuracy: 0.7619\n",
      "Epoch 175/500\n",
      "144/144 [==============================] - 0s 965us/sample - loss: 0.4877 - accuracy: 0.7361 - val_loss: 0.3880 - val_accuracy: 0.8095\n",
      "Epoch 176/500\n",
      "144/144 [==============================] - 0s 979us/sample - loss: 0.4343 - accuracy: 0.7708 - val_loss: 0.3491 - val_accuracy: 0.8571\n",
      "Epoch 177/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4357 - accuracy: 0.7986 - val_loss: 0.3565 - val_accuracy: 0.8095\n",
      "Epoch 178/500\n",
      "144/144 [==============================] - 0s 903us/sample - loss: 0.5142 - accuracy: 0.7708 - val_loss: 0.3940 - val_accuracy: 0.8095\n",
      "Epoch 179/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4278 - accuracy: 0.7917 - val_loss: 0.3697 - val_accuracy: 0.9048\n",
      "Epoch 180/500\n",
      "144/144 [==============================] - 0s 813us/sample - loss: 0.4843 - accuracy: 0.7431 - val_loss: 0.4248 - val_accuracy: 0.8095\n",
      "Epoch 181/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4506 - accuracy: 0.7917 - val_loss: 0.3725 - val_accuracy: 0.8571\n",
      "Epoch 182/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.5478 - accuracy: 0.6806 - val_loss: 0.3936 - val_accuracy: 0.8571\n",
      "Epoch 183/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4476 - accuracy: 0.7639 - val_loss: 0.3930 - val_accuracy: 0.8095\n",
      "Epoch 184/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4418 - accuracy: 0.8056 - val_loss: 0.3851 - val_accuracy: 0.7619\n",
      "Epoch 185/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4815 - accuracy: 0.7500 - val_loss: 0.4048 - val_accuracy: 0.8571\n",
      "Epoch 186/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4981 - accuracy: 0.7847 - val_loss: 0.4046 - val_accuracy: 0.8571\n",
      "Epoch 187/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4332 - accuracy: 0.8194 - val_loss: 0.3678 - val_accuracy: 0.7619\n",
      "Epoch 188/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.4623 - accuracy: 0.7986 - val_loss: 0.3448 - val_accuracy: 0.8571\n",
      "Epoch 189/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4665 - accuracy: 0.7708 - val_loss: 0.3535 - val_accuracy: 0.8571\n",
      "Epoch 190/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4878 - accuracy: 0.7222 - val_loss: 0.3691 - val_accuracy: 0.8571\n",
      "Epoch 191/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4554 - accuracy: 0.7708 - val_loss: 0.3894 - val_accuracy: 0.8571\n",
      "Epoch 192/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4602 - accuracy: 0.7847 - val_loss: 0.3821 - val_accuracy: 0.9048\n",
      "Epoch 193/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4612 - accuracy: 0.7778 - val_loss: 0.3523 - val_accuracy: 0.8095\n",
      "Epoch 194/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.4531 - accuracy: 0.7778 - val_loss: 0.3574 - val_accuracy: 0.9048\n",
      "Epoch 195/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.5118 - accuracy: 0.7639 - val_loss: 0.3801 - val_accuracy: 0.8571\n",
      "Epoch 196/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4387 - accuracy: 0.8056 - val_loss: 0.3568 - val_accuracy: 0.9048\n",
      "Epoch 197/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4710 - accuracy: 0.7986 - val_loss: 0.3366 - val_accuracy: 0.9048\n",
      "Epoch 198/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4654 - accuracy: 0.7639 - val_loss: 0.3601 - val_accuracy: 0.8571\n",
      "Epoch 199/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.4393 - accuracy: 0.7500 - val_loss: 0.3783 - val_accuracy: 0.8571\n",
      "Epoch 200/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4409 - accuracy: 0.8056 - val_loss: 0.3411 - val_accuracy: 0.8095\n",
      "Epoch 201/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4171 - accuracy: 0.8264 - val_loss: 0.3369 - val_accuracy: 0.8571\n",
      "Epoch 202/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4221 - accuracy: 0.7917 - val_loss: 0.3239 - val_accuracy: 0.8571\n",
      "Epoch 203/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4300 - accuracy: 0.7917 - val_loss: 0.3397 - val_accuracy: 0.9048\n",
      "Epoch 204/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4259 - accuracy: 0.8264 - val_loss: 0.3359 - val_accuracy: 0.8571\n",
      "Epoch 205/500\n",
      "144/144 [==============================] - 0s 785us/sample - loss: 0.3956 - accuracy: 0.8194 - val_loss: 0.3176 - val_accuracy: 0.8571\n",
      "Epoch 206/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.5262 - accuracy: 0.7222 - val_loss: 0.3772 - val_accuracy: 0.8095\n",
      "Epoch 207/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4575 - accuracy: 0.7708 - val_loss: 0.3705 - val_accuracy: 0.9048\n",
      "Epoch 208/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.5054 - accuracy: 0.7569 - val_loss: 0.3790 - val_accuracy: 0.9048\n",
      "Epoch 209/500\n",
      "144/144 [==============================] - 0s 750us/sample - loss: 0.4279 - accuracy: 0.8125 - val_loss: 0.3463 - val_accuracy: 0.8571\n",
      "Epoch 210/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4551 - accuracy: 0.8125 - val_loss: 0.3348 - val_accuracy: 0.8571\n",
      "Epoch 211/500\n",
      "144/144 [==============================] - 0s 840us/sample - loss: 0.4254 - accuracy: 0.8125 - val_loss: 0.3400 - val_accuracy: 0.8571\n",
      "Epoch 212/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4230 - accuracy: 0.7847 - val_loss: 0.3520 - val_accuracy: 0.8095\n",
      "Epoch 213/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4488 - accuracy: 0.7917 - val_loss: 0.3467 - val_accuracy: 0.9048\n",
      "Epoch 214/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4263 - accuracy: 0.8403 - val_loss: 0.3432 - val_accuracy: 0.9048\n",
      "Epoch 215/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4616 - accuracy: 0.7569 - val_loss: 0.3741 - val_accuracy: 0.8095\n",
      "Epoch 216/500\n",
      "144/144 [==============================] - 0s 771us/sample - loss: 0.4123 - accuracy: 0.7986 - val_loss: 0.3616 - val_accuracy: 0.8095\n",
      "Epoch 217/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.5217 - accuracy: 0.7431 - val_loss: 0.3756 - val_accuracy: 0.8095\n",
      "Epoch 218/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4958 - accuracy: 0.7708 - val_loss: 0.3948 - val_accuracy: 0.8095\n",
      "Epoch 219/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4257 - accuracy: 0.7292 - val_loss: 0.4025 - val_accuracy: 0.8571\n",
      "Epoch 220/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.5082 - accuracy: 0.7222 - val_loss: 0.3846 - val_accuracy: 0.9048\n",
      "Epoch 221/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4589 - accuracy: 0.7708 - val_loss: 0.3945 - val_accuracy: 0.8571\n",
      "Epoch 222/500\n",
      "144/144 [==============================] - 0s 882us/sample - loss: 0.4495 - accuracy: 0.7708 - val_loss: 0.3620 - val_accuracy: 0.8571\n",
      "Epoch 223/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4127 - accuracy: 0.7986 - val_loss: 0.3590 - val_accuracy: 0.8571\n",
      "Epoch 224/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3719 - accuracy: 0.8125 - val_loss: 0.3565 - val_accuracy: 0.8095\n",
      "Epoch 225/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.3960 - accuracy: 0.7847 - val_loss: 0.3133 - val_accuracy: 0.8571\n",
      "Epoch 226/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.5365 - accuracy: 0.7431 - val_loss: 0.3054 - val_accuracy: 0.9524\n",
      "Epoch 227/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.5304 - accuracy: 0.7014 - val_loss: 0.3724 - val_accuracy: 0.9048\n",
      "Epoch 228/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.4344 - accuracy: 0.8264 - val_loss: 0.3914 - val_accuracy: 0.7619\n",
      "Epoch 229/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4575 - accuracy: 0.7847 - val_loss: 0.3780 - val_accuracy: 0.7619\n",
      "Epoch 230/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.4301 - accuracy: 0.7917 - val_loss: 0.3057 - val_accuracy: 0.8571\n",
      "Epoch 231/500\n",
      "144/144 [==============================] - 0s 597us/sample - loss: 0.4651 - accuracy: 0.7847 - val_loss: 0.3135 - val_accuracy: 0.8571\n",
      "Epoch 232/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4151 - accuracy: 0.7847 - val_loss: 0.3619 - val_accuracy: 0.8095\n",
      "Epoch 233/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4632 - accuracy: 0.7847 - val_loss: 0.3695 - val_accuracy: 0.8095\n",
      "Epoch 234/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4408 - accuracy: 0.7917 - val_loss: 0.4002 - val_accuracy: 0.8095\n",
      "Epoch 235/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.5322 - accuracy: 0.7292 - val_loss: 0.3894 - val_accuracy: 0.9048\n",
      "Epoch 236/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.5306 - accuracy: 0.7222 - val_loss: 0.4027 - val_accuracy: 0.8571\n",
      "Epoch 237/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4787 - accuracy: 0.7708 - val_loss: 0.3943 - val_accuracy: 0.8095\n",
      "Epoch 238/500\n",
      "144/144 [==============================] - 0s 799us/sample - loss: 0.4419 - accuracy: 0.8125 - val_loss: 0.3824 - val_accuracy: 0.7619\n",
      "Epoch 239/500\n",
      "144/144 [==============================] - 0s 861us/sample - loss: 0.4099 - accuracy: 0.8125 - val_loss: 0.3702 - val_accuracy: 0.7619\n",
      "Epoch 240/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4942 - accuracy: 0.7917 - val_loss: 0.3499 - val_accuracy: 0.9048\n",
      "Epoch 241/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4803 - accuracy: 0.7431 - val_loss: 0.3427 - val_accuracy: 0.8095\n",
      "Epoch 242/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4894 - accuracy: 0.7639 - val_loss: 0.3391 - val_accuracy: 0.8095\n",
      "Epoch 243/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4996 - accuracy: 0.7569 - val_loss: 0.3659 - val_accuracy: 0.8571\n",
      "Epoch 244/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4190 - accuracy: 0.7847 - val_loss: 0.3705 - val_accuracy: 0.8095\n",
      "Epoch 245/500\n",
      "144/144 [==============================] - 0s 882us/sample - loss: 0.4154 - accuracy: 0.8194 - val_loss: 0.3616 - val_accuracy: 0.8095\n",
      "Epoch 246/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4109 - accuracy: 0.8056 - val_loss: 0.3711 - val_accuracy: 0.8571\n",
      "Epoch 247/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4023 - accuracy: 0.8333 - val_loss: 0.3443 - val_accuracy: 0.8095\n",
      "Epoch 248/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4207 - accuracy: 0.7778 - val_loss: 0.3380 - val_accuracy: 0.8571\n",
      "Epoch 249/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.3302 - val_accuracy: 0.8571\n",
      "Epoch 250/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.3850 - accuracy: 0.7917 - val_loss: 0.3140 - val_accuracy: 0.9048\n",
      "Epoch 251/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4700 - accuracy: 0.7917 - val_loss: 0.3210 - val_accuracy: 0.9048\n",
      "Epoch 252/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4352 - accuracy: 0.7986 - val_loss: 0.3494 - val_accuracy: 0.8095\n",
      "Epoch 253/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.3838 - accuracy: 0.7986 - val_loss: 0.3571 - val_accuracy: 0.8571\n",
      "Epoch 254/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4500 - accuracy: 0.7500 - val_loss: 0.3593 - val_accuracy: 0.8095\n",
      "Epoch 255/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.5012 - accuracy: 0.7431 - val_loss: 0.3508 - val_accuracy: 0.7619\n",
      "Epoch 256/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.4518 - accuracy: 0.7778 - val_loss: 0.3517 - val_accuracy: 0.8095\n",
      "Epoch 257/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4123 - accuracy: 0.8125 - val_loss: 0.3549 - val_accuracy: 0.8571\n",
      "Epoch 258/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4403 - accuracy: 0.7569 - val_loss: 0.3402 - val_accuracy: 0.8571\n",
      "Epoch 259/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4626 - accuracy: 0.7986 - val_loss: 0.3521 - val_accuracy: 0.8095\n",
      "Epoch 260/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.4152 - accuracy: 0.8333 - val_loss: 0.3323 - val_accuracy: 0.8095\n",
      "Epoch 261/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4187 - accuracy: 0.8472 - val_loss: 0.3373 - val_accuracy: 0.9048\n",
      "Epoch 262/500\n",
      "144/144 [==============================] - 0s 833us/sample - loss: 0.5222 - accuracy: 0.7361 - val_loss: 0.3441 - val_accuracy: 0.8571\n",
      "Epoch 263/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4122 - accuracy: 0.7639 - val_loss: 0.3260 - val_accuracy: 0.8571\n",
      "Epoch 264/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.5192 - accuracy: 0.7569 - val_loss: 0.3330 - val_accuracy: 0.8571\n",
      "Epoch 265/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4009 - accuracy: 0.8194 - val_loss: 0.3485 - val_accuracy: 0.8571\n",
      "Epoch 266/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4683 - accuracy: 0.7847 - val_loss: 0.3724 - val_accuracy: 0.8571\n",
      "Epoch 267/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3815 - accuracy: 0.8056 - val_loss: 0.3384 - val_accuracy: 0.9048\n",
      "Epoch 268/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4391 - accuracy: 0.7917 - val_loss: 0.3458 - val_accuracy: 0.8571\n",
      "Epoch 269/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.4497 - accuracy: 0.7569 - val_loss: 0.3590 - val_accuracy: 0.8571\n",
      "Epoch 270/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.4265 - accuracy: 0.7917 - val_loss: 0.3249 - val_accuracy: 0.9048\n",
      "Epoch 271/500\n",
      "144/144 [==============================] - 0s 819us/sample - loss: 0.4451 - accuracy: 0.7778 - val_loss: 0.3079 - val_accuracy: 0.9048\n",
      "Epoch 272/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.4477 - accuracy: 0.7986 - val_loss: 0.3333 - val_accuracy: 0.9048\n",
      "Epoch 273/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4148 - accuracy: 0.7986 - val_loss: 0.2972 - val_accuracy: 0.9048\n",
      "Epoch 274/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4317 - accuracy: 0.7847 - val_loss: 0.3220 - val_accuracy: 0.9048\n",
      "Epoch 275/500\n",
      "144/144 [==============================] - 0s 931us/sample - loss: 0.3675 - accuracy: 0.8472 - val_loss: 0.3385 - val_accuracy: 0.8571\n",
      "Epoch 276/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4861 - accuracy: 0.7292 - val_loss: 0.3486 - val_accuracy: 0.8571\n",
      "Epoch 277/500\n",
      "144/144 [==============================] - 0s 792us/sample - loss: 0.4031 - accuracy: 0.8194 - val_loss: 0.3641 - val_accuracy: 0.8571\n",
      "Epoch 278/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4962 - accuracy: 0.7778 - val_loss: 0.3456 - val_accuracy: 0.9048\n",
      "Epoch 279/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4723 - accuracy: 0.8056 - val_loss: 0.3629 - val_accuracy: 0.8571\n",
      "Epoch 280/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.4281 - accuracy: 0.7917 - val_loss: 0.3537 - val_accuracy: 0.9048\n",
      "Epoch 281/500\n",
      "144/144 [==============================] - 0s 813us/sample - loss: 0.3899 - accuracy: 0.8264 - val_loss: 0.3443 - val_accuracy: 0.9048\n",
      "Epoch 282/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4728 - accuracy: 0.7708 - val_loss: 0.3231 - val_accuracy: 0.9048\n",
      "Epoch 283/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.5106 - accuracy: 0.7431 - val_loss: 0.3542 - val_accuracy: 0.8571\n",
      "Epoch 284/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.5004 - accuracy: 0.7708 - val_loss: 0.3646 - val_accuracy: 0.8571\n",
      "Epoch 285/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.5734 - accuracy: 0.7222 - val_loss: 0.4297 - val_accuracy: 0.7619\n",
      "Epoch 286/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4338 - accuracy: 0.7847 - val_loss: 0.3474 - val_accuracy: 0.9048\n",
      "Epoch 287/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4707 - accuracy: 0.7569 - val_loss: 0.4049 - val_accuracy: 0.7619\n",
      "Epoch 288/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4597 - accuracy: 0.7639 - val_loss: 0.4174 - val_accuracy: 0.8095\n",
      "Epoch 289/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4263 - accuracy: 0.7847 - val_loss: 0.3676 - val_accuracy: 0.8571\n",
      "Epoch 290/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4150 - accuracy: 0.8542 - val_loss: 0.3333 - val_accuracy: 0.8095\n",
      "Epoch 291/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3947 - accuracy: 0.8403 - val_loss: 0.3306 - val_accuracy: 0.8571\n",
      "Epoch 292/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4744 - accuracy: 0.7708 - val_loss: 0.3735 - val_accuracy: 0.8571\n",
      "Epoch 293/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4583 - accuracy: 0.7778 - val_loss: 0.3656 - val_accuracy: 0.8571\n",
      "Epoch 294/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4135 - accuracy: 0.8194 - val_loss: 0.3520 - val_accuracy: 0.9048\n",
      "Epoch 295/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4697 - accuracy: 0.7778 - val_loss: 0.3670 - val_accuracy: 0.8571\n",
      "Epoch 296/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.3977 - accuracy: 0.7986 - val_loss: 0.3391 - val_accuracy: 0.8571\n",
      "Epoch 297/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.3370 - val_accuracy: 0.8571\n",
      "Epoch 298/500\n",
      "144/144 [==============================] - 2953s 21s/sample - loss: 0.4330 - accuracy: 0.7917 - val_loss: 0.3747 - val_accuracy: 0.8571\n",
      "Epoch 299/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4548 - accuracy: 0.7917 - val_loss: 0.3287 - val_accuracy: 0.8571\n",
      "Epoch 300/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.3986 - accuracy: 0.8264 - val_loss: 0.3220 - val_accuracy: 0.8571\n",
      "Epoch 301/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.3770 - accuracy: 0.8542 - val_loss: 0.3780 - val_accuracy: 0.8095\n",
      "Epoch 302/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4335 - accuracy: 0.7986 - val_loss: 0.3345 - val_accuracy: 0.8571\n",
      "Epoch 303/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4036 - accuracy: 0.7917 - val_loss: 0.3682 - val_accuracy: 0.8571\n",
      "Epoch 304/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.3791 - accuracy: 0.7917 - val_loss: 0.2952 - val_accuracy: 0.9524\n",
      "Epoch 305/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4006 - accuracy: 0.8125 - val_loss: 0.2851 - val_accuracy: 0.9048\n",
      "Epoch 306/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4695 - accuracy: 0.7639 - val_loss: 0.3980 - val_accuracy: 0.7619\n",
      "Epoch 307/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4979 - accuracy: 0.7569 - val_loss: 0.3598 - val_accuracy: 0.8095\n",
      "Epoch 308/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4230 - accuracy: 0.7847 - val_loss: 0.3308 - val_accuracy: 0.8571\n",
      "Epoch 309/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4941 - accuracy: 0.7639 - val_loss: 0.3630 - val_accuracy: 0.8095\n",
      "Epoch 310/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4202 - accuracy: 0.7917 - val_loss: 0.3496 - val_accuracy: 0.8095\n",
      "Epoch 311/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4611 - accuracy: 0.8264 - val_loss: 0.3501 - val_accuracy: 0.8571\n",
      "Epoch 312/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.3859 - val_accuracy: 0.8095\n",
      "Epoch 313/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4361 - accuracy: 0.8056 - val_loss: 0.3358 - val_accuracy: 0.8571\n",
      "Epoch 314/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4325 - accuracy: 0.7986 - val_loss: 0.3383 - val_accuracy: 0.8571\n",
      "Epoch 315/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.4310 - accuracy: 0.8333 - val_loss: 0.4074 - val_accuracy: 0.8095\n",
      "Epoch 316/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4624 - accuracy: 0.7778 - val_loss: 0.3592 - val_accuracy: 0.8095\n",
      "Epoch 317/500\n",
      "144/144 [==============================] - 0s 951us/sample - loss: 0.3756 - accuracy: 0.8403 - val_loss: 0.3448 - val_accuracy: 0.8095\n",
      "Epoch 318/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4687 - accuracy: 0.7500 - val_loss: 0.3634 - val_accuracy: 0.8571\n",
      "Epoch 319/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4625 - accuracy: 0.7847 - val_loss: 0.3526 - val_accuracy: 0.8095\n",
      "Epoch 320/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4747 - accuracy: 0.7361 - val_loss: 0.3622 - val_accuracy: 0.8095\n",
      "Epoch 321/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4255 - accuracy: 0.8403 - val_loss: 0.3872 - val_accuracy: 0.8095\n",
      "Epoch 322/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4458 - accuracy: 0.8056 - val_loss: 0.3361 - val_accuracy: 0.8571\n",
      "Epoch 323/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4703 - accuracy: 0.7500 - val_loss: 0.3023 - val_accuracy: 0.9524\n",
      "Epoch 324/500\n",
      "144/144 [==============================] - 0s 938us/sample - loss: 0.4425 - accuracy: 0.8125 - val_loss: 0.3196 - val_accuracy: 0.8571\n",
      "Epoch 325/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4195 - accuracy: 0.7917 - val_loss: 0.3453 - val_accuracy: 0.8571\n",
      "Epoch 326/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.3969 - accuracy: 0.8264 - val_loss: 0.3097 - val_accuracy: 0.9048\n",
      "Epoch 327/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4314 - accuracy: 0.7778 - val_loss: 0.3109 - val_accuracy: 0.8571\n",
      "Epoch 328/500\n",
      "144/144 [==============================] - 0s 750us/sample - loss: 0.4494 - accuracy: 0.8194 - val_loss: 0.3108 - val_accuracy: 0.8571\n",
      "Epoch 329/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4835 - accuracy: 0.7639 - val_loss: 0.3550 - val_accuracy: 0.8095\n",
      "Epoch 330/500\n",
      "144/144 [==============================] - 0s 854us/sample - loss: 0.4144 - accuracy: 0.8403 - val_loss: 0.3852 - val_accuracy: 0.8095\n",
      "Epoch 331/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4098 - accuracy: 0.7986 - val_loss: 0.3469 - val_accuracy: 0.8571\n",
      "Epoch 332/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4961 - accuracy: 0.7569 - val_loss: 0.3674 - val_accuracy: 0.8571\n",
      "Epoch 333/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4180 - accuracy: 0.8056 - val_loss: 0.4410 - val_accuracy: 0.8095\n",
      "Epoch 334/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.4627 - accuracy: 0.8194 - val_loss: 0.3912 - val_accuracy: 0.7619\n",
      "Epoch 335/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4537 - accuracy: 0.7847 - val_loss: 0.3567 - val_accuracy: 0.9048\n",
      "Epoch 336/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3999 - accuracy: 0.7986 - val_loss: 0.4169 - val_accuracy: 0.7619\n",
      "Epoch 337/500\n",
      "144/144 [==============================] - 0s 3ms/sample - loss: 0.4055 - accuracy: 0.8194 - val_loss: 0.3554 - val_accuracy: 0.9048\n",
      "Epoch 338/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3818 - accuracy: 0.8056 - val_loss: 0.2904 - val_accuracy: 0.9048\n",
      "Epoch 339/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4028 - accuracy: 0.8125 - val_loss: 0.3114 - val_accuracy: 0.8571\n",
      "Epoch 340/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.3752 - accuracy: 0.8472 - val_loss: 0.3959 - val_accuracy: 0.7619\n",
      "Epoch 341/500\n",
      "144/144 [==============================] - 0s 556us/sample - loss: 0.4034 - accuracy: 0.7986 - val_loss: 0.2705 - val_accuracy: 0.9524\n",
      "Epoch 342/500\n",
      "144/144 [==============================] - 0s 569us/sample - loss: 0.4360 - accuracy: 0.8125 - val_loss: 0.3100 - val_accuracy: 0.8095\n",
      "Epoch 343/500\n",
      "144/144 [==============================] - 0s 792us/sample - loss: 0.4142 - accuracy: 0.8194 - val_loss: 0.3043 - val_accuracy: 0.8095\n",
      "Epoch 344/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4128 - accuracy: 0.7847 - val_loss: 0.3239 - val_accuracy: 0.8571\n",
      "Epoch 345/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4748 - accuracy: 0.7361 - val_loss: 0.3412 - val_accuracy: 0.8571\n",
      "Epoch 346/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4966 - accuracy: 0.7431 - val_loss: 0.3191 - val_accuracy: 0.8095\n",
      "Epoch 347/500\n",
      "144/144 [==============================] - 0s 785us/sample - loss: 0.4236 - accuracy: 0.7778 - val_loss: 0.3576 - val_accuracy: 0.8095\n",
      "Epoch 348/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.4096 - accuracy: 0.7708 - val_loss: 0.3036 - val_accuracy: 0.8095\n",
      "Epoch 349/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4069 - accuracy: 0.7917 - val_loss: 0.2933 - val_accuracy: 0.8095\n",
      "Epoch 350/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4601 - accuracy: 0.7569 - val_loss: 0.3058 - val_accuracy: 0.9048\n",
      "Epoch 351/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4813 - accuracy: 0.7708 - val_loss: 0.3390 - val_accuracy: 0.8095\n",
      "Epoch 352/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4164 - accuracy: 0.7778 - val_loss: 0.3551 - val_accuracy: 0.8095\n",
      "Epoch 353/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4404 - accuracy: 0.7708 - val_loss: 0.3597 - val_accuracy: 0.8095\n",
      "Epoch 354/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4012 - accuracy: 0.8194 - val_loss: 0.3627 - val_accuracy: 0.8095\n",
      "Epoch 355/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3720 - accuracy: 0.8264 - val_loss: 0.3336 - val_accuracy: 0.8571\n",
      "Epoch 356/500\n",
      "144/144 [==============================] - 0s 819us/sample - loss: 0.3693 - accuracy: 0.8403 - val_loss: 0.3231 - val_accuracy: 0.8571\n",
      "Epoch 357/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4184 - accuracy: 0.7847 - val_loss: 0.3040 - val_accuracy: 0.8571\n",
      "Epoch 358/500\n",
      "144/144 [==============================] - 0s 583us/sample - loss: 0.3838 - accuracy: 0.8472 - val_loss: 0.3323 - val_accuracy: 0.8571\n",
      "Epoch 359/500\n",
      "144/144 [==============================] - 0s 750us/sample - loss: 0.4087 - accuracy: 0.7917 - val_loss: 0.3239 - val_accuracy: 0.8095\n",
      "Epoch 360/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.3518 - accuracy: 0.8125 - val_loss: 0.3220 - val_accuracy: 0.8571\n",
      "Epoch 361/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.3390 - accuracy: 0.8611 - val_loss: 0.3738 - val_accuracy: 0.7619\n",
      "Epoch 362/500\n",
      "144/144 [==============================] - 0s 563us/sample - loss: 0.4304 - accuracy: 0.8125 - val_loss: 0.3212 - val_accuracy: 0.8571\n",
      "Epoch 363/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4584 - accuracy: 0.8125 - val_loss: 0.3465 - val_accuracy: 0.7619\n",
      "Epoch 364/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4333 - accuracy: 0.7917 - val_loss: 0.3969 - val_accuracy: 0.7619\n",
      "Epoch 365/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4165 - accuracy: 0.8403 - val_loss: 0.3274 - val_accuracy: 0.8571\n",
      "Epoch 366/500\n",
      "144/144 [==============================] - 0s 799us/sample - loss: 0.4035 - accuracy: 0.7847 - val_loss: 0.3990 - val_accuracy: 0.8095\n",
      "Epoch 367/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4205 - accuracy: 0.8125 - val_loss: 0.3172 - val_accuracy: 0.9048\n",
      "Epoch 368/500\n",
      "144/144 [==============================] - 0s 583us/sample - loss: 0.4967 - accuracy: 0.7639 - val_loss: 0.3501 - val_accuracy: 0.8571\n",
      "Epoch 369/500\n",
      "144/144 [==============================] - 0s 792us/sample - loss: 0.4873 - accuracy: 0.7847 - val_loss: 0.3545 - val_accuracy: 0.8095\n",
      "Epoch 370/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4249 - accuracy: 0.7778 - val_loss: 0.3246 - val_accuracy: 0.8571\n",
      "Epoch 371/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.3820 - accuracy: 0.8056 - val_loss: 0.3047 - val_accuracy: 0.8571\n",
      "Epoch 372/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.3486 - accuracy: 0.8611 - val_loss: 0.2899 - val_accuracy: 0.9048\n",
      "Epoch 373/500\n",
      "144/144 [==============================] - 0s 813us/sample - loss: 0.3420 - accuracy: 0.8542 - val_loss: 0.2875 - val_accuracy: 0.8571\n",
      "Epoch 374/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3714 - accuracy: 0.8125 - val_loss: 0.3028 - val_accuracy: 0.8571\n",
      "Epoch 375/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.4343 - accuracy: 0.7917 - val_loss: 0.2645 - val_accuracy: 0.9524\n",
      "Epoch 376/500\n",
      "144/144 [==============================] - 0s 972us/sample - loss: 0.3393 - accuracy: 0.8681 - val_loss: 0.3285 - val_accuracy: 0.8571\n",
      "Epoch 377/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.3645 - accuracy: 0.8333 - val_loss: 0.3124 - val_accuracy: 0.8571\n",
      "Epoch 378/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.4225 - accuracy: 0.8194 - val_loss: 0.3217 - val_accuracy: 0.9048\n",
      "Epoch 379/500\n",
      "144/144 [==============================] - 0s 729us/sample - loss: 0.3907 - accuracy: 0.8264 - val_loss: 0.3308 - val_accuracy: 0.8571\n",
      "Epoch 380/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.3643 - accuracy: 0.7917 - val_loss: 0.3122 - val_accuracy: 0.9048\n",
      "Epoch 381/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4776 - accuracy: 0.7708 - val_loss: 0.3385 - val_accuracy: 0.8095\n",
      "Epoch 382/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4531 - accuracy: 0.7917 - val_loss: 0.3528 - val_accuracy: 0.8095\n",
      "Epoch 383/500\n",
      "144/144 [==============================] - 0s 778us/sample - loss: 0.4326 - accuracy: 0.7639 - val_loss: 0.3328 - val_accuracy: 0.9524\n",
      "Epoch 384/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4107 - accuracy: 0.7986 - val_loss: 0.3372 - val_accuracy: 0.8095\n",
      "Epoch 385/500\n",
      "144/144 [==============================] - 0s 792us/sample - loss: 0.4316 - accuracy: 0.8056 - val_loss: 0.3281 - val_accuracy: 0.8095\n",
      "Epoch 386/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4715 - accuracy: 0.7847 - val_loss: 0.3661 - val_accuracy: 0.8095\n",
      "Epoch 387/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4182 - accuracy: 0.8056 - val_loss: 0.3698 - val_accuracy: 0.8571\n",
      "Epoch 388/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4227 - accuracy: 0.7847 - val_loss: 0.3321 - val_accuracy: 0.9048\n",
      "Epoch 389/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.3830 - accuracy: 0.8194 - val_loss: 0.3205 - val_accuracy: 0.9048\n",
      "Epoch 390/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.3930 - accuracy: 0.8056 - val_loss: 0.3154 - val_accuracy: 0.9524\n",
      "Epoch 391/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.3932 - accuracy: 0.8194 - val_loss: 0.3036 - val_accuracy: 0.8571\n",
      "Epoch 392/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.4039 - accuracy: 0.7847 - val_loss: 0.3173 - val_accuracy: 0.9048\n",
      "Epoch 393/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4616 - accuracy: 0.7708 - val_loss: 0.3342 - val_accuracy: 0.8095\n",
      "Epoch 394/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4098 - accuracy: 0.8056 - val_loss: 0.4272 - val_accuracy: 0.7619\n",
      "Epoch 395/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4118 - accuracy: 0.7847 - val_loss: 0.3405 - val_accuracy: 0.8095\n",
      "Epoch 396/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4115 - accuracy: 0.8194 - val_loss: 0.3333 - val_accuracy: 0.8571\n",
      "Epoch 397/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.3707 - accuracy: 0.8472 - val_loss: 0.3839 - val_accuracy: 0.8095\n",
      "Epoch 398/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4217 - accuracy: 0.8056 - val_loss: 0.3591 - val_accuracy: 0.8095\n",
      "Epoch 399/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4376 - accuracy: 0.7708 - val_loss: 0.3755 - val_accuracy: 0.8095\n",
      "Epoch 400/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.3369 - accuracy: 0.8681 - val_loss: 0.3472 - val_accuracy: 0.8095\n",
      "Epoch 401/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.3595 - accuracy: 0.8403 - val_loss: 0.3249 - val_accuracy: 0.8571\n",
      "Epoch 402/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.3964 - accuracy: 0.7917 - val_loss: 0.3733 - val_accuracy: 0.8095\n",
      "Epoch 403/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4274 - accuracy: 0.7778 - val_loss: 0.3152 - val_accuracy: 0.8571\n",
      "Epoch 404/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4224 - accuracy: 0.8194 - val_loss: 0.3319 - val_accuracy: 0.8095\n",
      "Epoch 405/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4257 - accuracy: 0.7917 - val_loss: 0.3232 - val_accuracy: 0.8571\n",
      "Epoch 406/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3815 - accuracy: 0.8681 - val_loss: 0.3426 - val_accuracy: 0.8095\n",
      "Epoch 407/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.3503 - accuracy: 0.8125 - val_loss: 0.3054 - val_accuracy: 0.8571\n",
      "Epoch 408/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.4621 - accuracy: 0.7986 - val_loss: 0.3295 - val_accuracy: 0.8571\n",
      "Epoch 409/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.4244 - accuracy: 0.7847 - val_loss: 0.3206 - val_accuracy: 0.8571\n",
      "Epoch 410/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.3572 - accuracy: 0.8542 - val_loss: 0.2997 - val_accuracy: 0.9524\n",
      "Epoch 411/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4618 - accuracy: 0.7639 - val_loss: 0.3183 - val_accuracy: 0.9048\n",
      "Epoch 412/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3661 - accuracy: 0.8194 - val_loss: 0.3052 - val_accuracy: 0.9524\n",
      "Epoch 413/500\n",
      "144/144 [==============================] - 0s 792us/sample - loss: 0.3734 - accuracy: 0.8194 - val_loss: 0.3189 - val_accuracy: 0.8571\n",
      "Epoch 414/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4346 - accuracy: 0.7917 - val_loss: 0.3355 - val_accuracy: 0.8095\n",
      "Epoch 415/500\n",
      "144/144 [==============================] - 0s 819us/sample - loss: 0.3776 - accuracy: 0.8194 - val_loss: 0.3208 - val_accuracy: 0.8571\n",
      "Epoch 416/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4331 - accuracy: 0.8194 - val_loss: 0.3589 - val_accuracy: 0.7619\n",
      "Epoch 417/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.3971 - accuracy: 0.7917 - val_loss: 0.3515 - val_accuracy: 0.8095\n",
      "Epoch 418/500\n",
      "144/144 [==============================] - 0s 875us/sample - loss: 0.3868 - accuracy: 0.7917 - val_loss: 0.3305 - val_accuracy: 0.8095\n",
      "Epoch 419/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4376 - accuracy: 0.8056 - val_loss: 0.3466 - val_accuracy: 0.8571\n",
      "Epoch 420/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.3640 - accuracy: 0.8472 - val_loss: 0.3547 - val_accuracy: 0.8095\n",
      "Epoch 421/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.3927 - accuracy: 0.8264 - val_loss: 0.3501 - val_accuracy: 0.8095\n",
      "Epoch 422/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.3742 - accuracy: 0.8333 - val_loss: 0.3105 - val_accuracy: 0.8571\n",
      "Epoch 423/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.3288 - accuracy: 0.8333 - val_loss: 0.3489 - val_accuracy: 0.8095\n",
      "Epoch 424/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4017 - accuracy: 0.8264 - val_loss: 0.3573 - val_accuracy: 0.7619\n",
      "Epoch 425/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.3756 - accuracy: 0.8333 - val_loss: 0.3312 - val_accuracy: 0.8571\n",
      "Epoch 426/500\n",
      "144/144 [==============================] - 0s 771us/sample - loss: 0.4093 - accuracy: 0.7986 - val_loss: 0.3658 - val_accuracy: 0.7619\n",
      "Epoch 427/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.4901 - accuracy: 0.7500 - val_loss: 0.3540 - val_accuracy: 0.8095\n",
      "Epoch 428/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.4619 - accuracy: 0.7917 - val_loss: 0.4342 - val_accuracy: 0.7619\n",
      "Epoch 429/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4071 - accuracy: 0.7847 - val_loss: 0.3070 - val_accuracy: 0.8571\n",
      "Epoch 430/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4271 - accuracy: 0.7847 - val_loss: 0.3617 - val_accuracy: 0.8095\n",
      "Epoch 431/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3697 - accuracy: 0.8472 - val_loss: 0.3444 - val_accuracy: 0.8095\n",
      "Epoch 432/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4157 - accuracy: 0.7778 - val_loss: 0.3267 - val_accuracy: 0.8571\n",
      "Epoch 433/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.4050 - accuracy: 0.8125 - val_loss: 0.3888 - val_accuracy: 0.7619\n",
      "Epoch 434/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4299 - accuracy: 0.7986 - val_loss: 0.3816 - val_accuracy: 0.7619\n",
      "Epoch 435/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4197 - accuracy: 0.8056 - val_loss: 0.3716 - val_accuracy: 0.8095\n",
      "Epoch 436/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4029 - accuracy: 0.8056 - val_loss: 0.3679 - val_accuracy: 0.8095\n",
      "Epoch 437/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3844 - accuracy: 0.8403 - val_loss: 0.3354 - val_accuracy: 0.8571\n",
      "Epoch 438/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3993 - accuracy: 0.7917 - val_loss: 0.3973 - val_accuracy: 0.8095\n",
      "Epoch 439/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3982 - accuracy: 0.8333 - val_loss: 0.3428 - val_accuracy: 0.7619\n",
      "Epoch 440/500\n",
      "144/144 [==============================] - 0s 2ms/sample - loss: 0.3222 - accuracy: 0.8750 - val_loss: 0.3457 - val_accuracy: 0.7619\n",
      "Epoch 441/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.3872 - accuracy: 0.8264 - val_loss: 0.3467 - val_accuracy: 0.7619\n",
      "Epoch 442/500\n",
      "144/144 [==============================] - 0s 1ms/sample - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.3489 - val_accuracy: 0.7619\n",
      "Epoch 443/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.4712 - accuracy: 0.7639 - val_loss: 0.3481 - val_accuracy: 0.7619\n",
      "Epoch 444/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.3950 - accuracy: 0.8194 - val_loss: 0.4047 - val_accuracy: 0.7619\n",
      "Epoch 445/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4456 - accuracy: 0.8264 - val_loss: 0.3993 - val_accuracy: 0.7619\n",
      "Epoch 446/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4051 - accuracy: 0.8125 - val_loss: 0.3954 - val_accuracy: 0.7619\n",
      "Epoch 447/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.3474 - accuracy: 0.8472 - val_loss: 0.3459 - val_accuracy: 0.8571\n",
      "Epoch 448/500\n",
      "144/144 [==============================] - 0s 611us/sample - loss: 0.4039 - accuracy: 0.8333 - val_loss: 0.3459 - val_accuracy: 0.8095\n",
      "Epoch 449/500\n",
      "144/144 [==============================] - 0s 590us/sample - loss: 0.4261 - accuracy: 0.8125 - val_loss: 0.3648 - val_accuracy: 0.7619\n",
      "Epoch 450/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3397 - accuracy: 0.8819 - val_loss: 0.3329 - val_accuracy: 0.8095\n",
      "Epoch 451/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4519 - accuracy: 0.8125 - val_loss: 0.3793 - val_accuracy: 0.7619\n",
      "Epoch 452/500\n",
      "144/144 [==============================] - 0s 785us/sample - loss: 0.3532 - accuracy: 0.8125 - val_loss: 0.3397 - val_accuracy: 0.8571\n",
      "Epoch 453/500\n",
      "144/144 [==============================] - 0s 618us/sample - loss: 0.4300 - accuracy: 0.7639 - val_loss: 0.3991 - val_accuracy: 0.7619\n",
      "Epoch 454/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.3858 - accuracy: 0.8194 - val_loss: 0.3797 - val_accuracy: 0.8095\n",
      "Epoch 455/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.3803 - accuracy: 0.8264 - val_loss: 0.3129 - val_accuracy: 0.8571\n",
      "Epoch 456/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.4164 - accuracy: 0.8194 - val_loss: 0.3674 - val_accuracy: 0.8095\n",
      "Epoch 457/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4015 - accuracy: 0.8056 - val_loss: 0.3007 - val_accuracy: 0.9048\n",
      "Epoch 458/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.3857 - accuracy: 0.8056 - val_loss: 0.3303 - val_accuracy: 0.8571\n",
      "Epoch 459/500\n",
      "144/144 [==============================] - 0s 604us/sample - loss: 0.3499 - accuracy: 0.8403 - val_loss: 0.3454 - val_accuracy: 0.8095\n",
      "Epoch 460/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.3946 - accuracy: 0.8333 - val_loss: 0.3159 - val_accuracy: 0.8571\n",
      "Epoch 461/500\n",
      "144/144 [==============================] - 0s 625us/sample - loss: 0.4344 - accuracy: 0.8056 - val_loss: 0.3177 - val_accuracy: 0.8571\n",
      "Epoch 462/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3545 - accuracy: 0.8194 - val_loss: 0.4067 - val_accuracy: 0.8095\n",
      "Epoch 463/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.4187 - accuracy: 0.7847 - val_loss: 0.3518 - val_accuracy: 0.8571\n",
      "Epoch 464/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.4090 - accuracy: 0.8125 - val_loss: 0.3344 - val_accuracy: 0.8095\n",
      "Epoch 465/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.4403 - accuracy: 0.8056 - val_loss: 0.4341 - val_accuracy: 0.7619\n",
      "Epoch 466/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.3984 - accuracy: 0.8333 - val_loss: 0.3913 - val_accuracy: 0.8095\n",
      "Epoch 467/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3682 - accuracy: 0.8333 - val_loss: 0.3202 - val_accuracy: 0.8571\n",
      "Epoch 468/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.3977 - accuracy: 0.8194 - val_loss: 0.3637 - val_accuracy: 0.8095\n",
      "Epoch 469/500\n",
      "144/144 [==============================] - 0s 639us/sample - loss: 0.3917 - accuracy: 0.8125 - val_loss: 0.3979 - val_accuracy: 0.8095\n",
      "Epoch 470/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.4347 - accuracy: 0.7917 - val_loss: 0.4175 - val_accuracy: 0.8095\n",
      "Epoch 471/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3989 - accuracy: 0.7986 - val_loss: 0.4755 - val_accuracy: 0.7143\n",
      "Epoch 472/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.3729 - accuracy: 0.8333 - val_loss: 0.3809 - val_accuracy: 0.7619\n",
      "Epoch 473/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.3861 - accuracy: 0.8333 - val_loss: 0.4161 - val_accuracy: 0.7619\n",
      "Epoch 474/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.3736 - accuracy: 0.8264 - val_loss: 0.4299 - val_accuracy: 0.8095\n",
      "Epoch 475/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.3835 - accuracy: 0.8333 - val_loss: 0.3743 - val_accuracy: 0.8095\n",
      "Epoch 476/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3684 - accuracy: 0.8403 - val_loss: 0.3778 - val_accuracy: 0.8571\n",
      "Epoch 477/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.4009 - accuracy: 0.7917 - val_loss: 0.3516 - val_accuracy: 0.8571\n",
      "Epoch 478/500\n",
      "144/144 [==============================] - 0s 681us/sample - loss: 0.3679 - accuracy: 0.8125 - val_loss: 0.3125 - val_accuracy: 0.8571\n",
      "Epoch 479/500\n",
      "144/144 [==============================] - 0s 646us/sample - loss: 0.3902 - accuracy: 0.8194 - val_loss: 0.4523 - val_accuracy: 0.7619\n",
      "Epoch 480/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.4522 - accuracy: 0.7917 - val_loss: 0.3193 - val_accuracy: 0.9048\n",
      "Epoch 481/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4131 - accuracy: 0.7986 - val_loss: 0.2866 - val_accuracy: 0.9048\n",
      "Epoch 482/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.3771 - accuracy: 0.8264 - val_loss: 0.3338 - val_accuracy: 0.9048\n",
      "Epoch 483/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3524 - accuracy: 0.8472 - val_loss: 0.3168 - val_accuracy: 0.8571\n",
      "Epoch 484/500\n",
      "144/144 [==============================] - 0s 667us/sample - loss: 0.3841 - accuracy: 0.8264 - val_loss: 0.3689 - val_accuracy: 0.8095\n",
      "Epoch 485/500\n",
      "144/144 [==============================] - 0s 771us/sample - loss: 0.3798 - accuracy: 0.8056 - val_loss: 0.3118 - val_accuracy: 0.8571\n",
      "Epoch 486/500\n",
      "144/144 [==============================] - 0s 764us/sample - loss: 0.3726 - accuracy: 0.8264 - val_loss: 0.3267 - val_accuracy: 0.8571\n",
      "Epoch 487/500\n",
      "144/144 [==============================] - 0s 694us/sample - loss: 0.3481 - accuracy: 0.8403 - val_loss: 0.4313 - val_accuracy: 0.7143\n",
      "Epoch 488/500\n",
      "144/144 [==============================] - 0s 708us/sample - loss: 0.3965 - accuracy: 0.8264 - val_loss: 0.3513 - val_accuracy: 0.8095\n",
      "Epoch 489/500\n",
      "144/144 [==============================] - 0s 688us/sample - loss: 0.3875 - accuracy: 0.8194 - val_loss: 0.2972 - val_accuracy: 0.8571\n",
      "Epoch 490/500\n",
      "144/144 [==============================] - 0s 736us/sample - loss: 0.3969 - accuracy: 0.8264 - val_loss: 0.3247 - val_accuracy: 0.8571\n",
      "Epoch 491/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.3777 - accuracy: 0.8472 - val_loss: 0.3351 - val_accuracy: 0.8571\n",
      "Epoch 492/500\n",
      "144/144 [==============================] - 0s 819us/sample - loss: 0.3584 - accuracy: 0.8125 - val_loss: 0.3754 - val_accuracy: 0.8095\n",
      "Epoch 493/500\n",
      "144/144 [==============================] - 0s 660us/sample - loss: 0.4105 - accuracy: 0.8194 - val_loss: 0.3567 - val_accuracy: 0.8571\n",
      "Epoch 494/500\n",
      "144/144 [==============================] - 0s 722us/sample - loss: 0.4307 - accuracy: 0.7847 - val_loss: 0.3191 - val_accuracy: 0.8571\n",
      "Epoch 495/500\n",
      "144/144 [==============================] - 0s 701us/sample - loss: 0.3239 - accuracy: 0.8403 - val_loss: 0.3091 - val_accuracy: 0.8571\n",
      "Epoch 496/500\n",
      "144/144 [==============================] - 0s 674us/sample - loss: 0.4203 - accuracy: 0.7986 - val_loss: 0.3029 - val_accuracy: 0.8571\n",
      "Epoch 497/500\n",
      "144/144 [==============================] - 0s 715us/sample - loss: 0.3932 - accuracy: 0.8056 - val_loss: 0.3191 - val_accuracy: 0.8571\n",
      "Epoch 498/500\n",
      "144/144 [==============================] - 0s 743us/sample - loss: 0.3344 - accuracy: 0.8333 - val_loss: 0.3482 - val_accuracy: 0.8571\n",
      "Epoch 499/500\n",
      "144/144 [==============================] - 0s 632us/sample - loss: 0.4195 - accuracy: 0.7847 - val_loss: 0.3618 - val_accuracy: 0.8571\n",
      "Epoch 500/500\n",
      "144/144 [==============================] - 0s 653us/sample - loss: 0.4055 - accuracy: 0.8125 - val_loss: 0.3253 - val_accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, train_labels, epochs = 500, validation_data =(Validation_data, Validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/1 - 0s - loss: 0.3030 - accuracy: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39243797299473787, 0.8372093]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error_rate = model.evaluate(test_data, test_labels, verbose=2)\n",
    "test_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY KFOLD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    # create model write code below\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(90, activation = 'relu', input_shape = (train_data.shape[1],))) # number of fearures (60) are used as vector in input shape\n",
    "    # model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.add(Dense(90, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "    #model.add(Dense(30, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "    \n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    # Compile model, write code below\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [144, 43]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b5be31023f96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_baseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: %.2f%% (%.2f%%)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [144, 43]"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=25, batch_size=32, verbose=2)\n",
    "kfold = StratifiedKFold(n_splits=30, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, train_data, test_data, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [144, 43]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-67438028cbec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_baseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Result:/n'\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [144, 43]"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 25, batch_size = 32, verbose = 2)\n",
    "kfold = StratifiedKFold(n_splits = 30, shuffle = True, random_state = seed)\n",
    "results = cross_val_score(estimator, train_data, test_data, cv = kfold)\n",
    "print('Result:/n' (results.mean()* 100, result.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
